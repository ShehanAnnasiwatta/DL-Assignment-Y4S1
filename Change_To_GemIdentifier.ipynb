{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShehanAnnasiwatta/DL-Assignment-Y4S1/blob/Harith_CNN/Change_To_GemIdentifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NfxlJcXe8LpI"
      },
      "outputs": [],
      "source": [
        "! pip install numpy pandas matplotlib tensorflow opendatasets -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/lsind18/gemstones-images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHg4xoAO9-do",
        "outputId": "c087a983-154b-4847-d8eb-0b14c40c6fbc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./gemstones-images\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE =(256,256)\n",
        "\n",
        "train_data_dir=\"/content/gemstones-images/train\"\n",
        "test_data_dir=\"/content/gemstones-images/test\""
      ],
      "metadata": {
        "id": "Jqf6gd4q-m9z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_data_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    validation_split=0.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_data_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    validation_split=0.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(test_data_dir,\n",
        "                                                         batch_size=BATCH_SIZE,\n",
        "                                                         image_size=IMAGE_SIZE)\n",
        "\n",
        "class_names=train_data.class_names\n",
        "class_names\n",
        "\n",
        "# Print shapes to verify loading\n",
        "print(\"Train Data:\", train_data)\n",
        "print(\"Validation Data:\", validation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eto53PbM_Ppb",
        "outputId": "e8a8a90a-edb0-4312-f1b2-f3a47060eb28"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2856 files belonging to 87 classes.\n",
            "Using 2571 files for training.\n",
            "Found 2856 files belonging to 87 classes.\n",
            "Using 285 files for validation.\n",
            "Found 363 files belonging to 87 classes.\n",
            "Train Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
            "Validation Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch,label_batch in train_data.take(1):\n",
        "   print(image_batch.shape)\n",
        "   print(label_batch .shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra2Aj5tTBt97",
        "outputId": "78919be9-1b7a-40fa-bc0c-04473a73887a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 256, 256, 3)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=train_data.map(lambda x,y:(x/255.0,y))\n",
        "validation_data=validation_data.map(lambda x,y:(x/255.0,y))\n",
        "test_data=test_data.map(lambda x,y:(x/255.0,y))\n",
        "\n",
        "# Cache and Prefetch\n",
        "train_data = train_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_data = validation_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Print shapes to verify loading\n",
        "print(\"Train Data:\", train_data)\n",
        "print(\"Validation Data:\", validation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B392B2m6Cbpd",
        "outputId": "49a21a58-0782-41d3-bdec-d7844952c02f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
            "Validation Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "  [\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\",input_shape=(256,256,3)),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "  ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "lraW1PC3Ep23"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start to create the model\n",
        "model = tf.keras.models.Sequential()  # Create the model as Sequential\n",
        "\n",
        "\n",
        "# Adding the conventional layers\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu', input_shape=(256,256, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "# Flatten layer\n",
        "model.add(tf.keras.layers.Flatten())  # Flatten the input\n",
        "\n",
        "# Adding dropout after flattening\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "# Batch normalization after flattening\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))  # Dense layer with 128 neurons\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))  # Dense layer with 128 neurons\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))    # Dense layer with 32 neurons\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Use a smaller learning rate\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "# Compile the model (add this step)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Correct loss function for integer-encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Check the datasets\n",
        "print(\"Train Data:\", train_data)\n",
        "print(\"Validation Data:\", validation_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjaVikWwFVQ3",
        "outputId": "baf8d8be-58db-4741-a88c-303cc1378fbb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
            "Validation Data: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=1000,\n",
        "    validation_data=validation_data\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8AWjpMlIe3J",
        "outputId": "75f48874-d480-43f7-8751-03343577042d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 425ms/step - accuracy: 0.0156 - loss: nan - val_accuracy: 0.0105 - val_loss: nan\n",
            "Epoch 2/1000\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 295ms/step - accuracy: 0.0194 - loss: nan - val_accuracy: 0.0105 - val_loss: nan\n",
            "Epoch 3/1000\n",
            "\u001b[1m57/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 289ms/step - accuracy: 0.0202 - loss: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the performance matrices\n",
        "\n",
        "# Use CategoricalAccuracy for multi-class classification with one-hot encoded labels\n",
        "accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Precision and Recall can be used, but they are typically applied on a per-class basis\n",
        "precision = tf.keras.metrics.Precision()\n",
        "recall = tf.keras.metrics.Recall()\n"
      ],
      "metadata": {
        "id": "B_yaGAh9-zjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_data.as_numpy_iterator():\n",
        "    x, y = batch\n",
        "    # Predict the probability distribution for each class\n",
        "    yhat = model.predict(x)\n",
        "\n",
        "    # Convert predicted probabilities to class labels using argmax\n",
        "    yhat_class = np.argmax(yhat, axis=1)\n",
        "\n",
        "    # Update metrics using the true labels and predicted class labels\n",
        "    precision.update_state(y, yhat_class)\n",
        "    recall.update_state(y, yhat_class)\n",
        "    accuracy.update_state(y, yhat_class)"
      ],
      "metadata": {
        "id": "T47c_CQv_4f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load pre-trained VGG16 model + higher-level layers\n",
        "base_model = VGG16(input_shape=(256, 256, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top of it\n",
        "model = tf.keras.models.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data,\n",
        "                    epochs=1000,\n",
        "                    validation_data=validation_data)\n"
      ],
      "metadata": {
        "id": "cBtkOM8r9Fz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use opencv to read the image\n",
        "! pip install opencv-python"
      ],
      "metadata": {
        "id": "kVp_ENwbA7OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the open cv\n",
        "import cv2"
      ],
      "metadata": {
        "id": "TJ2CQHUnDRmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=cv2.imread('/content/images (1).jpg')\n",
        "plt.imshow(image)  #Show the testing image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r4K8Tvt2DZos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the image resize\n",
        "resized_image=tf.image.resize(image,IMAGE_SIZE)\n",
        "scaled_image=resized_image/255"
      ],
      "metadata": {
        "id": "0lQWgC7KD12c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create image as (1,128,128,3)\n",
        "np.expand_dims(scaled_image,0).shape\n"
      ],
      "metadata": {
        "id": "oDZv6VzcEUeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Expand the dimensions\n",
        "\n",
        "# Assuming y_hat is the output from a softmax layer (for multi-class classification)\n",
        "y_hat = model.predict(np.expand_dims(scaled_image, 0))\n",
        "\n",
        "# Get the index of the class with the highest probability\n",
        "predicted_class_index = np.argmax(y_hat, axis=-1)\n",
        "\n",
        "# Print the relevant class name based on the predicted index\n",
        "print(f'Predicted class: {class_names[predicted_class_index[0]]}')\n"
      ],
      "metadata": {
        "id": "EcmTIuQAEvUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}